---
title: "STAT 536 - Case Study 5"
author: "Sam Lee, Paul Crowley"
format: pdf
subtitle: "\\emph{Credit Card Fraud}"
abstract: "Fraud detection is an important function of banking and serves to protect both the bank and the client. The vast majority of credit card transactions are valid, and this imbalance makes it difficult to detect fraudulent transactions. However, it is still essential to identify real cases of fraud without flagging clients for valid everyday purchases. Using dimension-reduced credit transaction data from the bank, we build a random forest model tuned with optimal hyperparameters and trained on added synthetic fraudulent data to correctly identify real fraudulent transactions 99.9% of the time."
geometry: 
  - top=1in
  - left=1in
  - right=1in
  - bottom=1in
fontsize: 10pt
header-includes:
  - \usepackage{amsmath}
  - \usepackage{algorithm}
  - \usepackage{algpseudocode}
  - \usepackage{fancyhdr}
  - \usepackage{threeparttable}
  - \pagestyle{fancy}
  - \fancyfoot[C]{Sam Lee and Paul Crowley, MS in Statistics at Brigham Young University}
  - \fancyfoot[R]{\thepage}
  - \fancyhead{}
  - \renewcommand{\headrulewidth}{0pt}
  - \renewcommand{\footrulewidth}{0pt}
---

```{r setup, include=FALSE}
library(tidyverse)
library(caret)
library(knitr)
library(patchwork)
library(kableExtra)
library(pROC)

set.seed(536)

source("setup.R")
source("eda.R")
show_correlation()
```

## Introduction

Credit card fraud represents a significant financial burden, with estimated global losses reaching approximately $22 billion annually. In response, credit card companies employ advanced machine learning techniques to identify fraudulent transactions accurately. This report examines a dataset containing around 300,000 credit card transactions, of which only 492 are fraudulent, translating to a prevalence of approximately 0.1\%. Given the rare nature of fraudulent events, our objective is to develop a high-performing model that can detect fraud with precision, minimizing both financial losses and false positives, which could otherwise disrupt legitimate users.

![Correlation matrix between factors used in model specification. Note that all partial components (V1-V28) are, by construction, uncorrelated with each other.](credit-cor.png){#fig-cor}

Several potential challenges in the data may impact our analysis. Given the nature of the data, we will estimate a series of non-parametric binary classification models to predict whether a transcaction is fraudulent. The dataset’s extreme class imbalance raises concerns about model performance, especially with a tendency to misclassify rare fraudulent transactions. Additionally, while principal components enable dimensionality reduction, they also reduce interpretability since they lack direct transactional meaning---while we ignore this caveat in this analysis to prioritize prediction, the lack of interpretability means we are unable to come up with an \emph{a priori} non-linear specification for a parametric model to account for any non-linear trends in the data. Since these trends are unknown, we rely on non-parametric specifications that will better be able to capture unique interactions and non-linear effects. Each Pricinpal component, by construction, is uncorrelated with each other; however, this doesn't negate the possibility of \emph{Amount} being correlated with any of the partial components (See @fig-cor). In our analysis, we will evoke methods that are robust to multicolinearity as well as methods that are resilient to factors that aren't significantly meaningful.

```{r echo=F, width=8, fig.cap="Relationship between transaction amount and likelihood of fraud. Estimates given by a logistic regression model with $n=284,802$. Uncertainty estimates computed as the 95% quantile interval of $B=100$ bootstrapped estimations."}
#| label: fig-amount

visualize_amount()
```

A preliminary analysis reveals that as \emph{Amount} increases, fraud is more likely (See @fig-amount). However, we caution as interpreting this result as causal as there may be confounding effects unadjusted for. We also acknowledge the potential non-linearity in \emph{Amount} with respect to the likelihood of fraud (as captured by the uncertainty bounds in @fig-amount).

## Methodology

To identify fraudulent credit card transactions effectively, we consider two robust ensemble methods: Random Forest and Gradient Boosting. Both methods have demonstrated high predictive performance in classification tasks, particularly with imbalanced datasets, making them suitable for our analysis. We also consider a k-Nearest Neighbors (KNN) model as a baseline model to compare against the ensemble models.

1. Using a Random Forest (RF) model will help us eliminate potentially unnecessary factors that would otherwise fit noise in the data through \emph{bagging} (we describe the bagging method, as implemented through a RF model in the Appendex; see Algorithm \ref{alg:random_forest}). The RF model, by construction, will reduce the variance in our predictions. Additionally, since the RF model is an aggregation of decision trees, it will also help model the non-linear trends and complex interactions within our data. However, the extreme class imbalance may lead the RF model to underperform in identifying fraudulent transactions. In order to minimize this bias, we will tune fitting hyperparameters.

2. Similar to the Random Forest Model, Gradient Boosting (GB) uses a series of decision trees---by construction, this will help us model the complex and non-linear relationships in our data. However, in contrast from RF, rather than an \emph{aggregation} of independent decision trees fit on a random selection of the data, GB builds trees sequentially, each one focused on correcting the errors of its predecessor. In each iteration, the model minimizes a differentiable loss function by adding a new tree that fits the residuals of the combined ensemble from previous iterations. For binary classification, we specify the Gradient Boosting model as\footnote{Here we define $M$ as the total number of trees, $h^{(m)}(x)$ is the $m$-th decision tree in the sequence, and $\alpha_m$ is the learning rate to control the contribution of each tree.},

\begin{equation}
  \begin{aligned}
    \hat{f}_{GB}(x) = \sum_{m=1}^M\alpha_mh^{(m)}(x)
  \end{aligned}
\end{equation}

Where each new tree, $h^{(m)}(x)$, is trained\footnote{
We evaluate how well the new tree fits on the residuals of the current predictions by computing the negative gradient of the loss function, $L(y, \hat{f}(x))=-[y\log \hat{p}(x)+(1-y)\log (1-\hat{p}(x))]$ (that is using the binary cross-entropy loss for binary classification),
$$
g_i(m) = -\frac{\partial L(y_i, \hat{f}(x_i))}{\partial \hat{f}(x_i)}\bigg|_{\hat{f}(x)=\hat{f}_{m-1}(x)}
$$
These often-called "pseudo-residuals" (the vector of $g_i(m)$) is what the subsequent model is trained on.} on the residuals (error) of the current prediction ($\hat{f}_{m-1}$). Similar to the RF model, the GB model yields high accuracy, models complex patters, and through hyperparameter tuning, allows for control of model complexity. The ability to "learn" from it's mistakes allows it to reduce bias iteratively, although we caution against overfitting a model like GB due to its inherent decision tree structure and sensitivity to model hyperparameters.

## Model Evaluation

In the context of this problem, the criticality of accurately classifying credit card transactions cannot be overstated. Given the consequences, we consider false negative predictions—failing to identify fraudulent transactions—far more severe than false positives. To address this, we prioritize higher sensitivity over specificity, erring on the side of identifying potential fraud at the cost of increased false alarms\footnote{As such, we evaluate our models using the $F_{\beta=2}\text{-Score}$, that is, using $F_\beta=\frac{\left(1+\beta^2\right)TP}{\left(1+\beta^2\right)TP+\beta^2FN+FP}$}, where \emph{TP, FN,} and \emph{FP} represent the number of true positive, false negative, and false positive predictions committed by the model, respectively, where we set $\beta=2$.}.

Both the random forest and gradient boosting models acheived near perfect results during in-sample training with added synthetic minority class data in all performance metrics evaluated, but our primary focus was on the cross validated test predictions. The random forest model had very high sensitivity relative to the gradient boosting model, which is perhaps the most important consideration because it is more important to correctly identify fraudulent transactions than incorrectly identify valid transactions. The random forest also achieved much better positive predictive value, indicating that a higher proportion of predicted fraudulent transactions are actually fraudulent.

```{r, echo=FALSE}
metrics_from_csv <- read.csv("metrics.csv")
kable(metrics_from_csv, 
      caption = "Combined Metrics for Random Forest and XGBoost (In-sample and Out-of-sample)", 
      align = "c") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
  kable_styling(font_size = 9)
```

Given the reduced dimensions of the data that respect data privacy, interpretability was not much of a consideration in the model selection process. If a highly interpretable model like logistic regression were chosen for this fraud classification task, interpretations could only be made about the principal components rather than the the original variables. Thus, both proposed models, are designed with predictive performance in mind. Ultimately, the random forest model produced more desirable results for test predictions whereas the gradient boosting model produced subpar sensitivity, so the random forest was selected to accomplish research objectives.

The random forest model is specified below with hyperparameters selected through a randomized grid search and performance evaluated by F2 score to prioiritize high sensitivity over high specificity. Each parameter is represented in the equation with its description, allowing for classification of $x_0$, a new transaction data point. The model may seem complicated, but one tree was extracted for visualization in the next section for a more straightforward understanding of the model.

\begin{equation}
\hat{y}(x_0) =
\begin{cases} 
1 & \text{if } \frac{1}{B} \sum_{b=1}^B \hat{y}^b(x_0; m, S, D) \geq T, \\
0 & \text{otherwise.}
\end{cases}
\quad \text{where:}
\begin{aligned}
B & = 200 \quad \text{(Number of trees)}, \\
m & = 4 \quad \text{(Number of features at each split)}, \\
S & = 1 \quad \text{(Minimum node size)}, \\
f & = 0.6 \quad \text{(Proportion of samples used for each bootstrap)}, \\
D & = 15 \quad \text{(Maximum depth of each tree)}, \\
T & = 0.46 \quad \text{(Probability threshold for classification)}.
\end{aligned}
\end{equation}


The random forest model does not operate on any distributional assumptions. The model is robust to highly correlated data and unequal variance, and the model results do not depend on any such justifications. All features were kept in the model to maximize predictive performance, but each tree only classifies based on 4 features to ensure unique contributions to the ensemble.

## Results

As an ensemble model, Random Forest combines the outputs of multiple decision trees to generate robust predictions, making it difficult to fully visualize its overall operations. Each tree is built independently using a bootstrapped sample of the data, with splits determined by Gini impurity criteria that group similar data together, and only a random subset of features is considered at each split. However, individual trees can be extracted to illustrate the model’s decision-making process. The plot below demonstrates one such tree, showing how the data is split at each node and the predicted probabilities for validity and fraud at each step. While this provides insight into the workings of a single tree, the full predictive power comes from the collective votes of the whole forest.

![Individual Decision Tree Example](tree.png)

The random forest model does not provide quantifiable and interpretable parameters to analyze the impact of each partial component. However, metrics of variable importance and partial dependence plots can still be used to identify the most impactful variables. By permutating each partial component to eliminate any existing relationship with fraud, the decrease in accuracy can be compared to determine which partial components have the greatest effect on fraud. Partial dependence plots demonstrate how the predicted probability of fraud changes as the partial component increases. These plots are given below.

![Variable Importance and Partial Dependence for Most Impactful Components](VarImport.png){ width=700 height=1100 }

The random forest model does an excellent job identifying the fraudulent transactions to accomplish analysis goals. Given a transaction is fraudulent, the model can accurately detect it 99.9% of the time in tested cases. Of the five new transactions for which we do not know whether they are valid or fraudulent, the model predicts only the third one to be fraudulent. Based on the model's high sensitivity, it seems very unlikely that any of the other 5 cases are fraudulent.

This model can predict fraud with very high accuracy and emphasizes correctly predicting fraudulent transactions over correctly predicting valid transactions due to the high risks associated with fraud. The model is very complex, but its process is easily visualized, and important variables are also identifiable if a more inferential analysis were to be conducted in the future. This model demonstrably reduces the rates of undetected fraud at the bank.

## Conclusion

The analysis used a random forest model to predict fraud with great success. The model incorporated synthetic minority class data to counteract the drastic class imbalance used tuned hyperparameters to optimize predictive performance without overfitting. Given a fraudulent transaction, the model can identify it as such about 99.9% of the time. Of the 5 new transactions, the model predicted only the third one to be fraud. However, certain shortcomings were observed: Random Forest exhibited lower predictive accuracy given a valid transaction, leading to a higher rate of false positives. These results highlight the trade-off between sensitivity and specificity in imbalanced datasets. Additionally, the limited interpretability of the models could impede actionable insights to prevent fraud. To address these limitations, future work explore anomaly detection approaches to improve specificity and utilize interpretability tools such as SHAP to provide greater transparency.

\newpage

## Appendix

\begin{algorithm}[h!]
\caption{Random Forest Algorithm}
\label{alg:random_forest}
\begin{algorithmic}[1]
\State \textbf{Input:} Training data with \( n \) samples and \( p \) features
\State \textbf{Parameters:} Number of trees \( B \), number of features to consider at each split \( m \) (where \( m < p \))
\For{$b = 1, \dots, B$}
    \State Draw a bootstrapped sample of size \( n \) from the training data
    \State Grow a decision tree \( \mathcal{T}_b \) on this sample:
    \begin{enumerate}
        \item At each node, randomly select \( m \) features from the \( p \) available features
        \item Split on the best feature among the \( m \) chosen features (based on some criterion, e.g., Gini impurity for classification)
        \item Repeat until the stopping criterion is met (e.g., maximum depth or minimum node size)
    \end{enumerate}
\EndFor
\State \textbf{Prediction:} For a new observation \( x_0 \)
\begin{enumerate}
    \item For each tree \( b = 1, \dots, B \), obtain a prediction \( \hat{y}^b(x_0) \) by passing \( x_0 \) down tree \( \mathcal{T}_b \)
    \item For regression: average the predictions:
    \[
    \hat{y}(x_0) = \frac{1}{B} \sum_{b=1}^{B} \hat{y}^b(x_0)
    \]
    \item For classification: take the majority vote:
    \[
    \hat{y}(x_0) = \text{mode}(\hat{y}^1(x_0), \dots, \hat{y}^B(x_0))
    \]
\end{enumerate}
\end{algorithmic}
\end{algorithm}

## Teamwork
Sam Lee wrote the introduction, proposed models, appendix, and did most of the coding for model development. Paul Crowley wrote the model evaluation, results, abstract, and conclusion and implemented SMOTE.

---
title: "STAT 536 - Case Study 5"
author: "Sam Lee, Paul Crowley"
format: pdf
subtitle: "\\emph{Credit Card Fraud}"
geometry: 
  - top=1in
  - left=1in
  - right=1in
  - bottom=1in
fontsize: 10pt
abstract: ""
header-includes:
  - \usepackage{amsmath}
  - \usepackage{algorithm}
  - \usepackage{algpseudocode}
  - \usepackage{fancyhdr}
  - \usepackage{threeparttable}
  - \pagestyle{fancy}
  - \fancyfoot[C]{Sam Lee and Paul Crowley, MS in Statistics at Brigham Young University}
  - \fancyfoot[R]{\thepage}
  - \fancyhead{}
  - \renewcommand{\headrulewidth}{0pt}
  - \renewcommand{\footrulewidth}{0pt}
---

```{r setup, include=FALSE}
library(tidyverse)
library(caret)
library(knitr)
library(patchwork)
library(kableExtra)
library(pROC)

set.seed(536)

source("setup.R")
source("eda.R")
show_correlation()
```

## Introduction

Credit card fraud represents a significant financial burden, with estimated global losses reaching approximately $22 billion annually. In response, credit card companies employ advanced machine learning techniques to identify fraudulent transactions accurately. This report examines a dataset containing around 300,000 credit card transactions, of which only 492 are fraudulent, translating to a prevalence of approximately 0.1\%. Given the rare nature of fraudulent events, our objective is to develop a high-performing model that can detect fraud with precision, minimizing both financial losses and false positives, which could otherwise disrupt legitimate users.

![Correlation matrix between factors used in model specification. Note that all partial components (V1-V28) are, by construction, uncorrelated with each other.](credit-cor.png){#fig-cor}

Several potential challenges in the data may impact our analysis. Given the nature of the data, we will estimate a series of non-parametric binary classification models to predict whether a transcaction is fraudulent. The dataset’s extreme class imbalance raises concerns about model performance, especially with a tendency to misclassify rare fraudulent transactions. Additionally, while principal components enable dimensionality reduction, they also reduce interpretability since they lack direct transactional meaning---while we ignore this caveat in this analysis to prioritize prediction, the lack of interpretability means we are unable to come up with an \emph{a priori} non-linear specification for a parametric model to account for any non-linear trends in the data. Since these trends are unknown, we rely on non-parametric specifications that will better be able to capture unique interactions and non-linear effects. Each Pricinpal component, by construction, is uncorrelated with each other; however, this doesn't negate the possibility of \emph{Amount} being correlated with any of the partial components (See @fig-cor). In our analysis, we will evoke methods that are robust to multicolinearity as well as methods that are resilient to factors that aren't significantly meaningful.

```{r echo=F, width=8, fig.cap="Relationship between transaction amount and likelihood of fraud. Estimates given by a logistic regression model with $n=284,802$. Uncertainty estimates computed as the 95% quantile interval of $B=100$ bootstrapped estimations."}
#| label: fig-amount

visualize_amount()
```

A preliminary analysis reveals that as \emph{Amount} increases, fraud is more likely (See @fig-amount). However, we caution as interpreting this result as causal as there may be confounding effects unadjusted for. We also acknowledge the potential non-linearity in \emph{Amount} with respect to the likelihood of fraud (as captured by the uncertainty bounds in @fig-amount).

## Methodology

To identify fraudulent credit card transactions effectively, we consider two robust ensemble methods: Random Forest and Gradient Boosting. Both methods have demonstrated high predictive performance in classification tasks, particularly with imbalanced datasets, making them suitable for our analysis. We also consider a k-Nearest Neighbors (KNN) model as a baseline model to compare against the ensemble models.

1. A k-Nearest Neighbors (KNN) model classifies new observations based on the “k” most similar data points in the training set (See Algorithm in \ref{alg:weighted_knn}). Since the data contain uncorrelated principal components, similarity is evaluated based on Euclidean distance between observations. KNN is non-parametric, so no distributional assumptions are necessary to model fraud. KNN is sensitive to localities, allowing the model to capture complex, non-linear patterns. However, such sensitivity can cause the model to overfit the data. KNN can also struggle with the class imbalance present in the data, as the majority class tends to dominate nearest neighbors. To address these, we can tune hyperparameters such as the number of neighbors and weighting schemes of these neighbors to improve predictive performance. The KNN model cannot assess a variable's significance or relative importance, but this is of little concern given the inherent barrier to interpretability from dimension reduction.

2. Using a Random Forest (RF) model will help us eliminate potentially unnecessary factors that would otherwise fit noise in the data through \emph{bagging} (we describe the bagging method, as implemented through a RF model in the Appendex; see Algorithm \ref{alg:random_forest}). The RF model, by construction, will reduce the variance in our predictions. Additionally, since the RF model is an aggregation of decision trees, it will also help model the non-linear trends and complex interactions within our data. However, the extreme class imbalance may lead the RF model to underperform in identifying fraudulent transactions. In order to minimize this bias, we will tune fitting hyperparameters.

3. Similar to the Random Forest Model, Gradient Boosting (GB) uses a series of decision trees---by construction, this will help us model the complex and non-linear relationships in our data. However, in contrast from RF, rather than an \emph{aggregation} of indenpent decision trees fit on a random selection of the data, GB builds trees sequentially, each one focused on correcting the errors of its predecessor. In each iteration, the model minimizes a differentiable loss function by adding a new tree that fits the residuals of the combined ensemble from previous iterations. For binary classification, we specify the Gradient Boosting model as\footnote{Here we define $M$ as the total number of trees, $h^{(m)}(x)$ is the $m$-th decision tree in the sequence, and $\alpha_m$ is the learning rate to control the contribution of each tree.},

\begin{equation}
  \begin{aligned}
    \hat{f}_{GB}(x) = \sum_{m=1}^M\alpha_mh^{(m)}(x)
  \end{aligned}
\end{equation}

Where each new tree, $h^{(m)}(x)$, is trained\footnote{
We evaluate how well the new tree fits on the residuals of the current predictions by computing the negative gradient of the loss function, $L(y, \hat{f}(x))=-[y\log \hat{p}(x)+(1-y)\log (1-\hat{p}(x))]$ (that is using the binary cross-entropy loss for binary classification),
$$
g_i(m) = -\frac{\partial L(y_i, \hat{f}(x_i))}{\partial \hat{f}(x_i)}\bigg|_{\hat{f}(x)=\hat{f}_{m-1}(x)}
$$
These often-called "pseudo-residuals" (the vector of $g_i(m)$) is what the subsequent model is trained on.} on the residuals (error) of the current prediction ($\hat{f}_{m-1}$). Similar to the RF model, the GB model yields high accuracy, models complex patters, and through hyperparameter tuning, allows for control of model complexity. The ability to "learn" from it's mistakes allows it to reduce bias iteratively, although we caution against overfitting a model like GB due to its inherent decision tree structure and sensitivity to model hyperparameters.

## Model Evaluation

In the context of this problem, the criticality of accurately classifying credit card transactions cannot be overstated. Given the consequences, we consider false negative predictions—failing to identify fraudulent transactions—far more severe than false positives. To address this, we prioritize higher sensitivity over specificity, erring on the side of identifying potential fraud at the cost of increased false alarms\footnote{As such, we evaluate our models using the $F_{\beta=2}\text{-Score}$, that is, using $F_\beta=\frac{\left(1+\beta^2\right)TP}{\left(1+\beta^2\right)TP+\beta^2FN+FP}$}, where \emph{TP, FN,} and \emph{FP} represent the number of true positive, false negative, and false positive predictions committed by the model, respectively, where we set $\beta=2$.}.

Both the random forest and gradient boosting models acheived near perfect results during in-sample training with added synthetic minority class data in all performance metrics evaluated, but our primary focus was on the test predictions. The random forest model had very high sensitivity relative to the gradient boosting model, which is perhaps the most important consideration because it is more important to correctly identify fraudulent transactions than incorrectly identify valid transactions. The random forest also achieved much better positive predictive value, indicating that a higher proportion of predicted fraudulent transactions are actually fraudulent. 

```{r, echo=FALSE}
metrics_from_csv <- read.csv("combined_metrics_table.csv")
kable(metrics_from_csv, 
      caption = "Combined Metrics for Random Forest and XGBoost (In-sample and Out-of-sample)", 
      align = "c") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)
```

Given the reduced dimensions of the data that respect data privacy, interpretability was not much of a consideration in the model selection process. If a highly interpretable model like logistic regression were chosen for this fraud classification task, interpretations could only be made about the principal components rather than the the original variables. Thus, both proposed models, are designed with predictive performance in mind. Ultimately, the random forest model produced more desirable results for test predictions, and this model was chosen to accomplish research objectives.

The random forest model is specified below with hyperparameters selected through a randomized grid search and performance evaluated by F2 score to prioiritize high sensitivity over high specificity.

$$
\hat{y}(x_0) =
\begin{cases} 
1 & \text{if } \frac{1}{B} \sum_{b=1}^B \hat{y}^b(x_0) \geq T, \\
0 & \text{otherwise.}
\end{cases}


\text{Where:}
\begin{align*}
B & = 200 \quad \text{(Number of trees)}, \\
m & = 4 \quad \text{(Number of features at each split)}, \\
S & = 1 \quad \text{(Minimum node size)}, \\
f & = 0.6 \quad \text{(Proportion of samples used for each bootstrap)}, \\
D & = 15 \quad \text{(Maximum depth of each tree)}, \\
T & = 0.46 \quad \text{(Probability threshold for classification)}.
\end{align*}
$$

The random forest model does not operate on any distributional assumptions. The model results do not depend on any such justifications. All features were kept in the model to maximize predictive performance, but each tree only classifies based on 4 features to ensure unique contributions to the ensemble.

## Results

As an ensemble model, it is not possible to visualize the complete operations of the random forest. However, individual trees used in the ensemble can easily be extracted to demonstrate its iterative process. The plot below shows where one such tree split the data with predicted probabilities for validity and fraud at each step.

![Individual Decision Tree Example](tree.png)

The random forest model does not provide interpretable parameters to analyze the impact of each partial component. However, metrics of variable importance and partial dependence plots can still be used to identify the most impactful variables. By permutating each partial component to eliminate any existing relationship with fraud, the decrease in accuracy can be compared to determine which partial components have the greatest effect on fraud. Partial dependence plots demonstrate how the predicted probability of fraud changes as the partial component increases. These plots are given below.

![Variable Importance and Partial Dependence for Most Impactful Components](VarImport.png)

The random forest model does an excellent job identifying the fraudulent transactions. Given a transaction is fraudulent, the model can accurately detect it 99.9% of the time in tested cases. Of the five new transactions for which we do not know whether they are valid or fraudulent, the model predicts only the third one to be fraudulent. Based on the model's sensitivity, it seems very unlikely that any of the other cases are fraudulent.

This model can predict fraud with very high accuracy and emphasizes correctly predicting fraudulent transactions over correctly predicting valid transactions due to the high risks associated with fraud. The model is very complex, but its process is easily visualized, and important variables are also identifiable. 

\newpage

## Appendix

\begin{algorithm}[h!]
\caption{Random Forest Algorithm}
\label{alg:random_forest}
\begin{algorithmic}[1]
\State \textbf{Input:} Training data with \( n \) samples and \( p \) features
\State \textbf{Parameters:} Number of trees \( B \), number of features to consider at each split \( m \) (where \( m < p \))
\For{$b = 1, \dots, B$}
    \State Draw a bootstrapped sample of size \( n \) from the training data
    \State Grow a decision tree \( \mathcal{T}_b \) on this sample:
    \begin{enumerate}
        \item At each node, randomly select \( m \) features from the \( p \) available features
        \item Split on the best feature among the \( m \) chosen features (based on some criterion, e.g., Gini impurity for classification)
        \item Repeat until the stopping criterion is met (e.g., maximum depth or minimum node size)
    \end{enumerate}
\EndFor
\State \textbf{Prediction:} For a new observation \( x_0 \)
\begin{enumerate}
    \item For each tree \( b = 1, \dots, B \), obtain a prediction \( \hat{y}^b(x_0) \) by passing \( x_0 \) down tree \( \mathcal{T}_b \)
    \item For regression: average the predictions:
    \[
    \hat{y}(x_0) = \frac{1}{B} \sum_{b=1}^{B} \hat{y}^b(x_0)
    \]
    \item For classification: take the majority vote:
    \[
    \hat{y}(x_0) = \text{mode}(\hat{y}^1(x_0), \dots, \hat{y}^B(x_0))
    \]
\end{enumerate}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h!]
\caption{Weighted k-Nearest Neighbors Algorithm}
\label{alg:weighted_knn}
\begin{algorithmic}[1]
\State \textbf{Input:} Training data with \( n \) samples and \( p \) features
\State \textbf{Parameters:} Number of neighbors \( k \), Distance metric \( d \) (e.g., Euclidean, Mahalanobis), Weighting scheme \( w \) (e.g., uniform, distance-based)
\For{a new observation \( x_0 \)}
    \State Calculate the distance \( d(x_0, x_i) \) between \( x_0 \) and each training sample \( x_i \) in the dataset
    \State Identify the \( k \) nearest neighbors to \( x_0 \) by selecting the \( k \) samples with the smallest distances to \( x_0 \)
    \State Apply the weighting scheme \( w_i \) to each of the \( k \) neighbors:
    \begin{enumerate}
        \item For uniform weights, \( w_i = 1 \)
        \item For distance-based weights, \( w_i = \frac{1}{d(x_0, x_i)} \)
    \end{enumerate}
\EndFor
\State \textbf{Prediction:} For a new observation \( x_0 \)
\begin{enumerate}
    \item \textbf{For classification:}
    \begin{enumerate}
        \item Calculate weighted class frequencies:
            \[
            F_c = \sum_{i=1}^{k} \left( w_i \cdot \mathbb{I}(y_i = c) \right)
            \]
            where \( \mathbb{I}(y_i = c) \) is 1 if \( y_i = c \), and 0 otherwise.
        \item Predict the class for \( x_0 \) as:
            \[
            \hat{y}(x_0) = \arg \max_{c} F_c
            \]
    \end{enumerate}
    \item \textbf{For regression:}
    \begin{enumerate}
        \item Calculate the weighted average of the target values of the \( k \) neighbors:
            \[
            \hat{y}(x_0) = \frac{\sum_{i=1}^{k} w_i \cdot y_i}{\sum_{i=1}^{k} w_i}
            \]
    \end{enumerate}
\end{enumerate}
\end{algorithmic}
\end{algorithm}

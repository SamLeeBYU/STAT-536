---
title: "Case Study 2"
subtitle: "Rocky Mountain River Drainage"
author: "Sam Lee & Patric Platts"
format: pdf
editor: visual
geometry: 
  - top=1in
  - left=1in
  - right=1in
  - bottom=1in
---

```{r setup, include=FALSE, eval = TRUE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = F)
library(tidyverse)
library(pls)
library(glmnet)
library(maps)
library(latex2exp)
library(knitr)

source("to_latex.R")

rivers <- read_csv("Rivers.csv")
metadata <- read_csv("Metadata.csv") %>%
  mutate(
    descr = tools::toTitleCase(descr)
  )

set.seed(536)
```

```{r, include=F, pcr-model}
Y <- rivers$Metric
X <- rivers[,colnames(rivers)[2:length(colnames(rivers))]] %>% 
  as.matrix()
X <- scale(X)
X <- X[, colSums(is.na(X)) == 0]

svd_X <- svd(X)
U <- svd_X$u  # Left singular vectors (n x p)
D <- diag(svd_X$d)  # Diagonal matrix of singular values (p x p)
V <- svd_X$v  # Right singular vectors (p x p), also the principal directions (eigenvectors)

# Set cross-validation parameters
K <- 10  # Number of folds for cross-validation
n <- nrow(X)  # Number of observations
folds <- sample(rep(1:K, length.out = n))  # Randomly assign each observation to a fold
max_k <- min(ncol(X), nrow(X))-25  # Maximum number of components

# Initialize a vector to store the average RMSE for each value of k
rmse_cv <- numeric(max_k)

# Cross-validation loop for different values of k
for (k in 1:max_k) {
  
  # Vector to store the RMSE for each fold
  rmse_fold <- numeric(K)
  
  # Perform K-fold cross-validation
  for (fold in 1:K) {
    
    # Split the data into training and validation sets
    train_idx <- which(folds != fold)
    test_idx <- which(folds == fold)
    
    X_train <- X[train_idx, ]
    Y_train <- Y[train_idx]
    X_test <- X[test_idx, ]
    Y_test <- Y[test_idx]
    
    # Compute the first k principal components for training
    Z_train <- X_train %*% V[, 1:k]  # Use the first k principal components
    gamma_hat <- solve(crossprod(Z_train, Z_train)) %*% t(Z_train) %*% Y_train  # OLS estimate for gamma
    
    # Predict on the test set using the same k components
    Z_test <- X_test %*% V[, 1:k]  # Use the same first k components for testing
    Y_hat_test <- Z_test %*% gamma_hat
    
    # Compute the RMSE on the test set
    rmse_fold[fold] <- sqrt(mean((Y_test - Y_hat_test)^2))
  }
  
  # Compute the average RMSE across all folds for this k
  rmse_cv[k] <- mean(rmse_fold)
}

# Find the optimal k that minimizes the RMSE
optimal_k <- which.min(rmse_cv)

Z.k <- X %*% V[,1:optimal_k]
# 
gamma.hat <- solve(crossprod(Z.k, Z.k))%*%t(Z.k)%*%Y
# 
# #(1xp) vector of covariates
# pcr.coef <- V[,1:optimal_k]%*%gamma.hat %>% as.vector()
# #Most influential factors
# n.factors <- 30
# influential.factors <- colnames(X)[order(abs(pcr.coef), decreasing = TRUE)[1:n.factors]]

y.hat.pcr <- Z.k%*%gamma.hat

RMSE <- sqrt(mean((Y-Y.hat)^2))
R.squared <- 1-sum((Y-Y.hat)^2)/sum((Y-mean(Y))^2)
adjusted.R.squared <- 1 - (1-R.squared)*(length(Y)-1)/(length(Y)-optimal_k)

adjusted.R.squared
R.squared
```

```{r include=F, cross-validate}
rivers.pcr <- pcr(Y ~ X, ncomp=optimal_k, scale=F)
model = cv.glmnet(x=X,y=Y, alpha = 1)

#Store the optimal hyperparameter for future use
lasso.lambda <- model$lambda.1se

y.hat.lasso <- predict(model, newx = X)

#In sample RMSE
sqrt(mean((Y-y.hat.lasso)^2))
sqrt(mean((Y-y.hat.pcr)^2))

#Out of sample RMSE
lasso.loocv <- numeric(length(Y))
pcr.loocv <- numeric(length(Y))
for(i in 1:length(Y)){
  Y.subset <- Y[-i]
  X.subset <- X[-i,]
  
  model.subset = glmnet(x=X.subset,y=Y.subset, lambda = lasso.lambda, alpha = 1)
  
  X.0 <- X[i,,drop=F]
  
  lasso.loocv[i] <- predict(model.subset, newx=X.0)

  #PCR Model Fit
  
  # Compute the first k principal components for training
  Z <- X.subset %*% V[, 1:optimal_k]  # Use the first k principal components
  gamma_hat <- solve(crossprod(Z, Z)) %*% t(Z) %*% Y.subset  # OLS estimate for gamma
  
  Z.0 <- X.0 %*% V[, 1:optimal_k]
  pcr.loocv[i] <- Z.0 %*% gamma_hat
}

#LOOCV
sqrt(mean((Y-lasso.loocv)^2))
sqrt(mean((Y-pcr.loocv)^2))
```

```{r fig.width=8, fig.cap = "Scatter plot of spatial displacement of each observation of recorded river flow", echo=F, map}
# Get the map of the US
us_map <- map_data("state")

# Create the map and scatter plot
ggplot() +
  # Plot the map of the US
  geom_polygon(data = us_map, aes(x = long, y = lat, group = group), fill = "white", color = "black") +
  # Add scatter plot of the observations
  geom_point(data = rivers, aes(x = Lon, y = Lat), color = "#AACCEE", size = 2) +
  # Set the map boundaries
  coord_fixed(1.3) +
  # Add labels and title
  labs(
       x = TeX("Longitude"), 
       y = TeX("Latitude")) +
  theme_minimal()
```

## 1

After standardizing the numerical covariates, we will fit two variable selection models: We will fit a lasso penalized regression model and a partial component regression (PCR) model. After fitting our models, we will apply a level of k-fold cross-validation to select the best hyperparameters for the model of best fit. After selecting our chosen model, we would examine the covariates that are the largest in absolute value (since the model would fit on scaled vectors) and which are significant.

## 2

We would look at the adjusted $R^2$ statistic as an overall model fit adjusted for the number of covariates included. We would also compare how well our selected covariates from the lasso regression compare to the full model fit by running an ANOVA test of the two models (comparing their f-statistics of significance).

## 3

To assess the predictive power of the model, we would look at the (out-of-sample) RMSE through k-fold cross-validation. To specifically look at the predictive power of the covariates we included, for a model like PCR, we could assess the portion of variance explained by each component as well as the total cumulative explained variance[^1]. For our lasso regression model, in addition to RMSE as assessed through cross-validation and the adjusted $R^2$, we would create ($B$ \# of) bootstraps of the data and fit the lasso model on each bootstrap, checking the frequency at which each covariate is selected[^2]. We can also use bootstrap to obtain confidence intervals for each coefficient as see if these are statisticaly significant.

[^1]: We would assess this using the criteria established by the following expression: $\sum_{j=1}^k\frac{\lambda_j^2}{\sum_{i=1}^p\lambda_i^2}$, where each $\lambda_j$ corresponds to the singular value of the $j$-th principal component.

[^2]: We can assess the selection frequency of each $j$-th covariate by using this bootstrapping method as described through the following expression: $\frac{1}{B}\sum_{b=1}^B\mathbb{1}(\hat{\beta}_j^{\text{lasso, } b} \neq 0)$.

## Introduction

Ecosystems are shaped by various factors, with rivers playing a critical role in distributing water and nutrients essential for plant and animal life. In the U.S. Rocky Mountains, the stability of river flow is particularly important, as it influences soil fertility—a key factor for agriculture that sustains both people and livestock. This analysis examines the factors affecting river flow in the Rocky Mountain Region, focusing on human activity, river network characteristics, and climate influences. The data used in this study were collected from multiple rivers in the region to explore how these variables impact overall water flow.

#### Insert a plot or illustration of the data?????

The number of covariates in the data is nearly equal to the number of observations, creating a risk of overfitting due to insufficient local information. In such cases, a standard linear regression model tends to overfitting, capturing noise rather than meaningful patterns, and resulting in poor model performance. High-dimensional data can also lead to "false positives," where unrelated variables appear to be associated with the response variable. To address these issues, this analysis applies variable selection and dimensionality reduction techniques. These methods will help identify the most significant factors influencing water flow in rivers throughout the Rocky Mountains.

## Methodology

To reduce potential colinearity between the different factors in the data set and arrive at an optimal parsimonious model, we propose two models to assess the overall water flow of water sources in the Rocky Mountains. In this section, we will discuss both candidate models and how these models can be used to answer the research questions at hand.

We first propose a Partial Component Regression (PCR) model. PCR combines Principal Component Analysis (PCA) with linear regression. Under the assumption that the parameters of interest ($\beta$) are linear—that is, assuming a one-unit increase in a $p$th factor (among those we consider) implies a $\beta_p$ increase in the water flow metric—we leverage this by applying linear regression to the set of orthogonal components computed by PCA[^3]. We used ten-fold cross-validation to select the most optimal number of components, $k^*$; through this process, we chose $k^*=9$. The strengths with $PCR$ come with its robustness to multicolinearity in the covariate matrix, $X$. Additionally, PCR performs dimensionality reduction by only selecting the top principal components (in our case, we selected 9) to achieve a parsimonious model.

[^3]: We first orthogonalize the set of all factors of interest, $X$, through singular-value decomposition, where $X=U\Sigma V'$. We then compute $Z_k=XV_k$, for $k$ number of components where $V_k$ is a subset of $V$ consisting of the first $k$ columns of $V$. Each column of $Z_k$ is then orthogonal to each other, that is, $Z_i'Z_j=0$ $\forall i\neq j$. Then, performing linear regression, we compute the set of linear $\gamma_k$ coefficient parameters (where $\gamma_k$ is of dimension $k$) using $Z_k$ as the new covariate matrix. Solving for $\gamma_k$, $\gamma_k=(Z_k'Z_k)^{-1}Z_k'Y$. $\hat{Y}$ is then computed as $\hat{Y}=Z_k\gamma_k$.

The tradeoff that comes with using PCR, however, is its lack of interpretability. Since each component is a linear combination of all individual covariates in $X$, the coefficients derived from our PCR model are not directly interpretable. Additionally, PCR computes and therefore selects components based on the variance of the covariate matrix $X$, as opposed to each factor's relationship with our response variable, the metric of water flow. Hence, the components may not necessarily contribute to predicting the outcome of interest.

Secondly, we propose fitting a Lasso Linear Regression model to accomplish both dimension reduction through variable selection and interpretability. Similar to our PCR model, we will operate on the assume that each of our factors have a linear effect on the water flow metric. However, after standardization on the matrix $X$, Lasso Regression imposes an $L_1$ penalty[^4] to both shrink the estimated coefficients and perform variable selection. Our Lasso Regression model is also suited to handle multicolinearity through the penalization parameter. However, unlike PCR, we can focus on predictive power since there is a direct relationship between the water flow metric and its covariates. Hence, we believe this model to be more interpretable.

[^4]: Formally, the $L_1$ penalty is computed as a vector norm ($||\cdot||$), where, for a vector $\beta$ with dimension $P$, $||\beta||=\sum_{p=1}^{P}|\beta_p|$.

When we introduce the penalty parameter, however, the coefficients on this model will be biased. We sacrifice this bias however for a decrease in the variance of the parameters. As a result, to accurately assess the standard error of each covariate effect, we perform bootstrapping methods to estimate 95% confidence intervals on $\beta$.

#### Model Evaluation

Both models, LASSO and PCR, were evaluated using in-sample and out-of-sample performance measures. To assess the in-sample fit, the adjusted $R^2$ was used. LASSO achieved an adjusted $R^2$ of [insert value], while PCR achieved an adjusted $R^2$ of [insert value]. This indicates that [LASSO/PCR] provides a better in-sample fit to the data.

For out-of-sample prediction performance, the root mean square error (RMSE) was compared between the two models. LASSO had an RMSE of [insert value], while PCR had an RMSE of [insert value]. Although both models yielded similar RMSE values, indicating that either method could be appropriate for analyzing this dataset, LASSO was ultimately chosen due to its interpretability.

PCR reduces dimensionality by grouping variables into components, each explaining a portion of the variability. However, the complexity of interpreting these components—since different variables have varying weights within each component—makes it difficult to determine the individual contribution of each factor to river water flow. In contrast, LASSO simplifies interpretation by shrinking less important variables to zero, leaving only the key factors that directly impact water flow.


$$
\underset{\beta}{\text{arg min}} \sum_{i=1}^n(y_i-x_i'\beta)^2+\lambda\sum_{p=1}^{P}|\beta_p|
$$

#### Talk about the assumptions of the model clearly

## Results

```{r echo=F, bootstrap}
lasso.covariates = rownames(coef(model))[as.vector(abs(coef(model)) > 0)]

#Bootstrap
B = 10000
K = 100
beta.b <- matrix(nrow=(1+ncol(X)), ncol=B)
for(b in 1:B){
  observations.b <- sample(1:length(Y), size=K, replace = T)
  Y.b <- Y[observations.b]
  X.b <- X[observations.b,]
  lasso.b <- glmnet(X.b, Y.b, lambda=lasso.lambda, alpha=1)
  beta.b[,b] <- as.vector(coef(lasso.b))
}
#How often each covariate is included
beta.importance <- (abs(beta.b) > 0) %>% ifelse(1, 0) %>% rowMeans()

beta.bar <- beta.b %>% rowMeans()
se.bar <- numeric(nrow(beta.b))
for(j in 1:nrow(beta.b)){
  se.bar[j] <- sum((beta.b[j,]-beta.bar[j])^2)
}
#Bootstrap standard errors
lasso.se <- sqrt((1/(B-1))*se.bar)

covariates.indices <- c(1, 1+(which(colnames(X) %in% lasso.covariates)))

#How often coefficients were included in B bootstraps
#beta.importance[covariates.indices]

#95% Centered Confidence Intervals
lasso.beta <- as.vector(coef(model))
beta.ci.centered.lower <- 2 * lasso.beta - apply(beta.b, 1, function(x) quantile(x, probs = 0.975))
beta.ci.centered.upper <- 2 * lasso.beta - apply(beta.b, 1, function(x) quantile(x, probs = 0.025))

beta.ci.centered <- cbind(beta.ci.centered.lower, beta.ci.centered.upper)
#beta.ci.centered[covariates.indices,]

to_latex(beta.ci.centered[covariates.indices,], 
                         lasso.beta[covariates.indices], 
                         beta.importance[covariates.indices],
        lasso.covariates, metadata
        )
```

```{r graph, echo=F, fig.width = 8, fig.align='center', fig.width=8, message=F, fig.cap="A comparison of actual and predicted values using Lasso Regression"}

bio15 = X[,"bio15"]

lasso.data <- data.frame(
  y = Y,              
  y.hat = y.hat.lasso,
  bio15.significant = abs(bio15) >= quantile(bio15, 0.95)
)
colnames(lasso.data)[2] <- "y.hat"

lasso.data %>%
  ggplot(aes(x = y.hat, y = y)) +
  geom_smooth(method = "lm", 
              formula = y ~ x,
              se = FALSE, color = "#111", linetype = "dashed",
              linewidth=0.5) + 
  geom_point(aes(color = bio15.significant), size = 2) +
  scale_color_manual(values = c("FALSE" = "#0072B2", "TRUE" = "#CC6699"),
                     name = "Significant\nbio15") +           
  labs(
    y = expression(Y),                      
    x = expression(hat(Y)),                    
    title = "Observed vs. Predicted Values"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    panel.grid.major = element_line(color = "gray80"),
    panel.grid.minor = element_blank(),
    legend.position = "inside",
    legend.position.inside = c(0.1, 0.85),
    legend.background = element_rect(color = "black", linewidth = 0.5),
    legend.box.background = element_rect(color = "black") 
  )
```

Make a table with coefficients, description, effects, and confidence interval of bootstrap distributions

```{r echo=F, in_sample}
Y.hat <- predict(model, newx = X)
R.squared <- 1-sum((Y-Y.hat)^2)/sum((Y-mean(Y))^2)
adjusted.R.squared <- 1 - (1-R.squared)*(length(Y)-1)/(length(Y)-length(lasso.covariates))
```

For in-sample fit, we computed an adjusted-R\^2 of....

Using LOOCV, we computed an out-of-sample RMSE of....So on average the out-of-sample prediction is \[LOOCV RMSE\] away from the actual measurement of river flow.

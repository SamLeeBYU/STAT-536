---
title: "Case Study 2"
author: "Sam Lee & Patric Platts"
format: pdf
editor: visual
---

```{r setup, include=FALSE, eval = TRUE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = F)
library(tidyverse)
library(pls)
library(glmnet)
library(maps)
library(latex2exp)

rivers <- read_csv("Rivers.csv")

set.seed(536)
```

```{r, include=F, model}
Y <- rivers$Metric
X <- rivers[,colnames(rivers)[2:length(colnames(rivers))]] %>% 
  as.matrix()
X <- scale(X)
X <- X[, colSums(is.na(X)) == 0]

svd_X <- svd(X)
U <- svd_X$u  # Left singular vectors (n x p)
D <- diag(svd_X$d)  # Diagonal matrix of singular values (p x p)
V <- svd_X$v  # Right singular vectors (p x p), also the principal directions (eigenvectors)

# Set cross-validation parameters
K <- 5  # Number of folds for cross-validation
n <- nrow(X)  # Number of observations
folds <- sample(rep(1:K, length.out = n))  # Randomly assign each observation to a fold
max_k <- min(ncol(X), nrow(X))-25  # Maximum number of components

# Initialize a vector to store the average RMSE for each value of k
rmse_cv <- numeric(max_k)

# Cross-validation loop for different values of k
for (k in 1:max_k) {
  
  # Vector to store the RMSE for each fold
  rmse_fold <- numeric(K)
  
  # Perform K-fold cross-validation
  for (fold in 1:K) {
    
    # Split the data into training and validation sets
    train_idx <- which(folds != fold)
    test_idx <- which(folds == fold)
    
    X_train <- X[train_idx, ]
    Y_train <- Y[train_idx]
    X_test <- X[test_idx, ]
    Y_test <- Y[test_idx]
    
    # Compute the first k principal components for training
    Z_train <- X_train %*% V[, 1:k]  # Use the first k principal components
    gamma_hat <- solve(crossprod(Z_train, Z_train)) %*% t(Z_train) %*% Y_train  # OLS estimate for gamma
    
    # Predict on the test set using the same k components
    Z_test <- X_test %*% V[, 1:k]  # Use the same first k components for testing
    Y_hat_test <- Z_test %*% gamma_hat
    
    # Compute the RMSE on the test set
    rmse_fold[fold] <- sqrt(mean((Y_test - Y_hat_test)^2))
  }
  
  # Compute the average RMSE across all folds for this k
  rmse_cv[k] <- mean(rmse_fold)
}

# Find the optimal k that minimizes the RMSE
optimal_k <- which.min(rmse_cv)

Z.k <- X %*% V[,1:optimal_k]
# 
gamma.hat <- solve(crossprod(Z.k, Z.k))%*%t(Z.k)%*%Y
# 
# #(1xp) vector of covariates
# pcr.coef <- V[,1:optimal_k]%*%gamma.hat %>% as.vector()
# #Most influential factors
# n.factors <- 30
# influential.factors <- colnames(X)[order(abs(pcr.coef), decreasing = TRUE)[1:n.factors]]

Y.hat <- Z.k%*%gamma.hat
# 
RMSE <- sqrt(mean((Y-Y.hat)^2))
# R.squared <- 1-sum((Y-Y.hat)^2)/sum((Y-mean(Y))^2)
# adjusted.R.squared <- 1 - (1-R.squared)*(length(Y)-1)/(length(Y)-optimal_k)
# 
# adjusted.R.squared
# R.squared
```

```{r}
rivers.pcr <- pcr(Y ~ X, ncomp=optimal_k, scale=F)
lasso.cv = cv.glmnet(x=X,y=Y, alpha = 1)
lasso.lambda <- lasso.cv$lambda.1se
#rownames(coef(lasso.cv))[as.vector(abs(coef(lasso.cv)) > 0)]

y.hat.lasso <- predict(lasso.cv, newx = X)

#In sample RMSE
sqrt(mean((Y-y.hat.lasso)^2))
sqrt(mean((Y-Y.hat)^2))

#Out of sample RMSE
lasso.loocv <- numeric(length(Y))
pcr.loocv <- numeric(length(Y))
for(i in 1:length(Y)){
  Y.subset <- Y[-i]
  X.subset <- X[-i,]
  
  lasso.cv = glmnet(x=X.subset,y=Y.subset, lambda = lasso.lambda, alpha = 1)
  
  X.0 <- X[i,,drop=F]
  
  lasso.loocv[i] <- predict(lasso.cv, newx=X.0)

  #PCR Model Fit
  
  # Compute the first k principal components for training
  Z <- X.subset %*% V[, 1:optimal_k]  # Use the first k principal components
  gamma_hat <- solve(crossprod(Z, Z)) %*% t(Z) %*% Y.subset  # OLS estimate for gamma
  
  Z.0 <- X.0 %*% V[, 1:optimal_k]
  pcr.loocv[i] <- Z.0 %*% gamma_hat
}

#LOOCV
sqrt(mean((Y-lasso.loocv)^2))
sqrt(mean((Y-pcr.loocv)^2))
```
```{r fig.width=8}
# Get the map of the US
us_map <- map_data("state")

# Create the map and scatter plot
ggplot() +
  # Plot the map of the US
  geom_polygon(data = us_map, aes(x = long, y = lat, group = group), fill = "white", color = "black") +
  # Add scatter plot of the observations
  geom_point(data = rivers, aes(x = Lon, y = Lat), color = "#AACCEE", size = 2, alpha = 0.6) +
  # Set the map boundaries
  coord_fixed(1.3) +
  # Add labels and title
  labs(title = TeX("Scatter Plot of River Observations"),
       x = TeX("Longitude"), 
       y = TeX("Latitude")) +
  theme_minimal()
```

## 1

After standardizing the numerical covariates, we will fit two variable selection models: We will fit a lasso penalized regression model and a partial component regression (PCR) model. After fitting our models, we will apply a level of k-fold cross-validation to select the best hyperparameters for the model of best fit. After selecting our chosen model, we would examine the covariates that are the largest in absolute value (since the model would fit on scaled vectors) and which are significant.

## 2

We would look at the adjusted $R^2$ statistic as an overall model fit adjusted for the number of covariates included. We would also compare how well our selected covariates from the lasso regression compare to the full model fit by running an ANOVA test of the two models (comparing their f-statistics of significance).

## 3

To assess the predictive power of the model, we would look at the (out-of-sample) RMSE through k-fold cross-validation. To specifically look at the predictive power of the covariates we included, for a model like PCR, we could assess the portion of variance explained by each component as well as the total cumulative explained variance[^1]. For our lasso regression model, in addition to RMSE as assessed through cross-validation and the adjusted $R^2$, we would create ($B$ \# of) bootstraps of the data and fit the lasso model on each bootstrap, checking the frequency at which each covariate is selected[^2]. We can also use bootstrap to obtain confidence intervals for each coefficient as see if these are statisticaly significant.

[^1]: We would assess this using the criteria established by the following expression:  $\sum_{j=1}^k\frac{\lambda_j^2}{\sum_{i=1}^p\lambda_i^2}$, where each $\lambda_j$ corresponds to the singular value of the $j$-th principal component.

[^2]: We can assess the selection frequency of each $j$-th covariate by using this bootrstrapping method as described through the following expression: $\frac{1}{B}\sum_{b=1}^B\mathbb{1}(\hat{\beta}_j^{\text{lasso, } b} \neq 0)$.
